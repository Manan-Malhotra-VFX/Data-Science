{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1715fdda",
   "metadata": {},
   "source": [
    "### all installation commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6imyDqdzTZ4E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6imyDqdzTZ4E",
    "outputId": "afb42cc0-8d03-4781-edbb-a610116ff413"
   },
   "outputs": [],
   "source": [
    "#!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "418b4126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47334279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e6f7b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7ebe8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The filename, directory name, or volume label syntax is incorrect.\n"
     ]
    }
   ],
   "source": [
    "pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c9c4077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The filename, directory name, or volume label syntax is incorrect.\n"
     ]
    }
   ],
   "source": [
    "pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "389a6f50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15708/1965156877.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ffd22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org torch===0.4.1 torchvision===0.4.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a1c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71206c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f64532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a19b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch-1.2.0-cp36-cp36m-manylinux1_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y h5py==2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66419cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abc5552",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Reading pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6efb68",
   "metadata": {
    "hidden": true,
    "id": "1d6efb68"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "\n",
    "import PyPDF2\n",
    "#import slate3k as slate\n",
    "import datetime as date\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "from spacy import displacy\n",
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from afinn import Afinn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.util import ngrams\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd2755",
   "metadata": {
    "hidden": true,
    "id": "1acd2755"
   },
   "outputs": [],
   "source": [
    "# creating a pdf file object to read pdf file\n",
    "#pdf_file_obj = open('/content/gdrive/MyDrive/Colab Notebooks/Ecommerce_Business_Guide.pdf','rb') # 5 pages done\n",
    "pdf_file_obj = open('Ecommerce_Business_Guide.pdf','rb') # 5 pages done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fda25e",
   "metadata": {
    "hidden": true,
    "id": "92fda25e"
   },
   "outputs": [],
   "source": [
    "#creating pdf filereader object\n",
    "pdf_reader = PyPDF2.PdfFileReader(pdf_file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa10a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "daaa10a0",
    "outputId": "e4663a8b-490e-41d6-bec6-2688e084c1f1"
   },
   "outputs": [],
   "source": [
    "def displayInfoBook(pdf_reader):\n",
    "    information = pdf_reader.getDocumentInfo()\n",
    "    print(\"Author:\",information.author)\n",
    "    print(\"Creator:\",information.creator)\n",
    "    print(\"Producer:\",information.producer)\n",
    "    print(\"Subject:\",information.subject)\n",
    "    print(\"Title:\",information.title)\n",
    "    print(\"Number of Pages:\",pdf_reader.getNumPages())\n",
    "      \n",
    "displayInfoBook(pdf_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b7d6f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "f9b7d6f7",
    "outputId": "04326e9d-61e5-4bf2-9d97-9bd20163e9be",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#reads data from all pages of pdf book\n",
    "no_pages = pdf_reader.getNumPages()\n",
    "\n",
    "start = date.datetime.now()\n",
    "corpus = ''\n",
    "for i in range(0, no_pages):\n",
    "    page = pdf_reader.getPage(i)\n",
    "    corpus += page.extractText()\n",
    "\n",
    "end = date.datetime.now()\n",
    "pdf_file_obj.close()\n",
    "\n",
    "print(\"Time taken =\",end-start)\n",
    "print(\"length of corpus =\",len(corpus))\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abff4ac",
   "metadata": {
    "heading_collapsed": true,
    "id": "6abff4ac"
   },
   "source": [
    "### Data Cleaning/ Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be56e62b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "hidden": true,
    "id": "be56e62b",
    "outputId": "2a757d61-7bfd-47d6-9b76-68c7966b7565"
   },
   "outputs": [],
   "source": [
    "corpus = corpus.replace(\"'s\",'') # replaces apostrophe s\n",
    "corpus = corpus.replace('\\n','') # replaces newline character\n",
    "corpus = re.sub(r'\\([^()]*\\)','',corpus) # removes text inside brackets including brackets\n",
    "corpus = re.sub(r'(http|https|www)\\S+', '', corpus) # replaces www.digitalsherpa.com,http://www.articlesbase.com/technology\n",
    "corpus = re.sub(r'\\<.+\\>','',corpus) # replaces <link rel=ﬂcanonicalﬂ href=ﬂﬂ />\n",
    "corpus = re.sub(r'\\s+',' ',corpus) # replaces more than 2 spaces with 1 space\n",
    "corpus = corpus.lower() # converts the text to lower\n",
    "\n",
    "print(\"corpus length =\",len(corpus))\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58eb735",
   "metadata": {
    "heading_collapsed": true,
    "id": "d58eb735"
   },
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc733afc",
   "metadata": {
    "hidden": true,
    "id": "dc733afc"
   },
   "source": [
    "#### Using Spacy & NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MDt9HlRkUMnG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "MDt9HlRkUMnG",
    "outputId": "f3c17eea-d88a-4fdb-fbb2-ddce7becf463"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a7c5b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "79a7c5b9",
    "outputId": "499c3384-ca4d-47c9-c75b-63f6bb2b8d1f"
   },
   "outputs": [],
   "source": [
    "#creating list of sentences\n",
    "list_sentences = sent_tokenize(corpus)\n",
    "print(\"Total no of sentences = \",len(list_sentences),\"\\n\")\n",
    "list_10 = list_sentences[0:10]\n",
    "list_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23910fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "hidden": true,
    "id": "d23910fc",
    "outputId": "e27fd438-d207-4009-ee88-34522b90fed5"
   },
   "outputs": [],
   "source": [
    "#Named entity recognition for sentences between 80 to 88\n",
    "ner = spacy.load('en_core_web_sm')\n",
    "\n",
    "for i in range(80,88):\n",
    "    one_sent = list_sentences[i]\n",
    "    doc_block = ner(one_sent)\n",
    "    displacy.render(doc_block, style='ent',  jupyter=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f970bd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hidden": true,
    "id": "1f970bd1",
    "outputId": "030986c0-978c-4946-f72e-2fad044c0c1d"
   },
   "outputs": [],
   "source": [
    "#Named entity recognition for entire text\n",
    "\n",
    "colNames = ['TEXT', 'LABEL', 'MEANING'] \n",
    "Entities_table = pd.DataFrame() #initialising dataframe\n",
    "\n",
    "ner = spacy.load('en_core_web_sm')\n",
    "ner_data = ner(corpus)\n",
    "for word in ner_data.ents:\n",
    "    varLabelValue =  word.label_\n",
    "    varLabelMeaning = spacy.explain(varLabelValue)\n",
    "    Entities_table = Entities_table.append(pd.DataFrame(data=[[word.text, word.label_, varLabelMeaning]],\n",
    "                                                        columns = colNames))\n",
    "    #print(word.text, word.label_, word.lemma_)\n",
    "    \n",
    "    \n",
    "Entities_table.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7af574",
   "metadata": {
    "hidden": true,
    "id": "aa7af574"
   },
   "source": [
    "* Text: The original word text.\n",
    "* Lemma: The base form of the word.\n",
    "* POS: The simple UPOS part-of-speech tag.\n",
    "* Tag: The detailed part-of-speech tag.\n",
    "* Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "* is alpha: Is the token an alpha character?\n",
    "* is stop: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b353e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "hidden": true,
    "id": "5d3b353e",
    "outputId": "91bf0b8a-2cab-474e-ff8e-6c7d5fe6aa27",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# POS and Dependency on entire corpus (string)\n",
    "#creating table with all values\n",
    "#creating noun and verb dictionary with key as text and values as their frequency\n",
    "\n",
    "colnames = ['TEXT', 'LEMMA', 'POS','TAGS','DEP','ALPHA','STOP']\n",
    "tags_pos_table = pd.DataFrame()\n",
    "\n",
    "noun_frequencies = {} # dictionary intialization for noun\n",
    "verb_frequencies = {} # dictionary intialization for verb\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "ner_data = nlp(corpus)\n",
    "for token in ner_data:\n",
    "    tags_pos_table = tags_pos_table.append(pd.DataFrame(\n",
    "        data=[[token.text,token.lemma_,token.pos_, token.tag_, token.dep_,token.is_alpha,token.is_stop]],\n",
    "                         columns = colnames))\n",
    "        #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.is_alpha, token.is_stop)\n",
    "    \n",
    "    #create dictionary of noun frequencies\n",
    "    if token.pos_ == 'NOUN':\n",
    "        if token.text not in noun_frequencies.keys():\n",
    "            noun_frequencies[token.text] = 1\n",
    "        else:    \n",
    "            noun_frequencies[token.text] +=1 \n",
    "    \n",
    "    #creates dictionary of verb frequencies        \n",
    "    if token.pos_ == 'VERB':\n",
    "        if token.text not in verb_frequencies.keys():\n",
    "            verb_frequencies[token.text] = 1\n",
    "        else:    \n",
    "            verb_frequencies[token.text] +=1                        \n",
    "                \n",
    "tags_pos_table.reset_index(drop=True, inplace=True)\n",
    "print(tags_pos_table.shape)\n",
    "tags_pos_table.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ed8a8",
   "metadata": {
    "hidden": true,
    "id": "c35ed8a8"
   },
   "outputs": [],
   "source": [
    "# converting noun and verb dictionary to dataframe for sorting and plotting\n",
    "\n",
    "list_noun_key = list(noun_frequencies.keys()) #converts dictionary keys to list of keys\n",
    "list_noun_val = list(noun_frequencies.values()) # converts dictionary values to list of values\n",
    "list_verb_key = list(verb_frequencies.keys())\n",
    "list_verb_val = list(verb_frequencies.values())\n",
    "\n",
    "# creating table for noun words frequency\n",
    "noun_table = pd.DataFrame()\n",
    "noun_table['noun']       = list_noun_key\n",
    "noun_table['noun_count'] = list_noun_val\n",
    "noun_table.sort_values(by='noun_count', ascending=False, inplace=True)\n",
    "noun_table_10            = noun_table.head(10)\n",
    "\n",
    "#creating table for verb word frequency\n",
    "verb_table               = pd.DataFrame()\n",
    "verb_table['verb']       = list_verb_key\n",
    "verb_table['verb_count'] = list_verb_val\n",
    "verb_table.sort_values(by='verb_count', ascending=False, inplace=True)\n",
    "verb_table_10            = verb_table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ea062",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "hidden": true,
    "id": "912ea062",
    "outputId": "2227f9eb-1b98-41f6-a169-3297652566cf"
   },
   "outputs": [],
   "source": [
    "noun_table_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad4680e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "hidden": true,
    "id": "fad4680e",
    "outputId": "686dabad-6ce3-4b19-ad24-546977385fe4"
   },
   "outputs": [],
   "source": [
    "fig = px.bar(noun_table_10, x='noun', y='noun_count',\n",
    "             color='noun_count',\n",
    "             height=400, title='Top 10 most common nouns')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d59a0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "hidden": true,
    "id": "f0d59a0e",
    "outputId": "0d954b6c-a3cd-4f75-860e-b615deecc416"
   },
   "outputs": [],
   "source": [
    "verb_table_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26880d66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "hidden": true,
    "id": "26880d66",
    "outputId": "b876c716-82db-4d8e-ce6c-c7235f02c2ed"
   },
   "outputs": [],
   "source": [
    "fig = px.bar(verb_table_10, x='verb', y='verb_count',\n",
    "             color='verb_count',\n",
    "             height=400, title='Top 10 most common verbs',\n",
    "             color_continuous_scale=px.colors.diverging.Tealrose,                          \n",
    "            )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd70fe3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "9cd70fe3",
    "outputId": "1dd1c2b6-8f3a-4a47-9dca-5cc76e88ae82"
   },
   "outputs": [],
   "source": [
    "# from nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#stop_words.add('testingstopwordsaddition')\n",
    "type(stop_words)\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac2061b",
   "metadata": {
    "hidden": true,
    "id": "7ac2061b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4675be8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "4675be8f",
    "outputId": "8d8e4bed-c9e4-408b-df93-ec4103013953"
   },
   "outputs": [],
   "source": [
    "#creating word_token from corpus\n",
    "print(\"corpus length =\",len(corpus))\n",
    "word_tokens = word_tokenize(corpus)\n",
    "print(\"lenght of tokens =\",len(word_tokens))\n",
    "print(word_tokens[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d5ff32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "b5d5ff32",
    "outputId": "ac9be043-2969-4087-f5e5-d6786f42ade4"
   },
   "outputs": [],
   "source": [
    "#removing punctuation, stop words\n",
    "filtered_tokens = []\n",
    "for word in word_tokens:\n",
    "    if word not in punctuation:\n",
    "        if word not in stop_words:\n",
    "            filtered_tokens.append(word)\n",
    "\n",
    "print(\"length of tokens =\",len(filtered_tokens))            \n",
    "print(filtered_tokens[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f55cae8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "1f55cae8",
    "outputId": "37475cf9-6f52-475d-c810-14c29a8aad91"
   },
   "outputs": [],
   "source": [
    "# most 10 common words\n",
    "# creating dictionaries of keys as words and values as frequency of that word\n",
    "word_frequencies = {} \n",
    "for word in filtered_tokens:\n",
    "    if word not in word_frequencies.keys():\n",
    "        word_frequencies[word] = 1\n",
    "    else:\n",
    "        word_frequencies[word] +=1\n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d94317",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "37d94317",
    "outputId": "1b361f38-c458-4b28-f26f-193559c0d74f"
   },
   "outputs": [],
   "source": [
    "max(word_frequencies.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56e8dda",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "hidden": true,
    "id": "e56e8dda",
    "outputId": "388befc3-335e-4ff2-f44f-0b0c45e51b47"
   },
   "outputs": [],
   "source": [
    "list_keys = list(word_frequencies.keys())\n",
    "list_values = list(word_frequencies.values())\n",
    "#list_keys\n",
    "#list_values\n",
    "#converting the dictionary to dataframe\n",
    "#so that sorting and plotting will become easier\n",
    "wordcount_table = pd.DataFrame()\n",
    "wordcount_table['words'] = list_keys\n",
    "wordcount_table['wordcount'] = list_values\n",
    "word_count_t = wordcount_table.sort_values(by='wordcount', ascending=False).head(10)\n",
    "word_count_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a0608",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "hidden": true,
    "id": "906a0608",
    "outputId": "4223e789-3104-4e26-bc95-27e2c57d89b5"
   },
   "outputs": [],
   "source": [
    "fig = px.bar(word_count_t, x='words', y='wordcount',\n",
    "             color='wordcount',\n",
    "             height=400, title='Top 10 most common words',\n",
    "             color_continuous_scale=px.colors.sequential.Viridis             \n",
    "            )\n",
    "fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f005076",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "3f005076",
    "outputId": "8b28e6a3-a85d-4273-b947-9af61070fb42"
   },
   "outputs": [],
   "source": [
    "#bigrams = ngrams(corpus.split(), 2)\n",
    "#for item in bigrams:\n",
    "#    print(item)\n",
    "\n",
    "#creating dicitionary of bigrams with their frequency\n",
    "bigrams = ngrams(corpus.split(), 2)\n",
    "\n",
    "bigram_dict = {}\n",
    "for item in bigrams:\n",
    "    #print(item)\n",
    "    itemText = item[0]+' '+item[1]\n",
    "    if itemText not in bigram_dict.keys():\n",
    "        bigram_dict[itemText] = 1\n",
    "    else:    \n",
    "        bigram_dict[itemText] +=1 \n",
    "    \n",
    "bigram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1ed5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "hidden": true,
    "id": "7ff1ed5e",
    "outputId": "f1b37755-2dab-49c2-afe9-f14b71177a47"
   },
   "outputs": [],
   "source": [
    "#converting dictionary to dataframe for sorting and plotting\n",
    "list_bigram_key = list(bigram_dict.keys()) #converts dictionary keys to list of keys\n",
    "list_bigram_val = list(bigram_dict.values()) # converts dictionary values to list of values\n",
    "\n",
    "# creating table for Bigrams\n",
    "bigram_table = pd.DataFrame()\n",
    "bigram_table['Bigrams']    = list_bigram_key\n",
    "bigram_table['Frequency']  = list_bigram_val\n",
    "bigram_table.sort_values(by='Frequency', ascending=False, inplace=True)\n",
    "bigram_table_10            = bigram_table.head(10)\n",
    "bigram_table_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def4042e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "hidden": true,
    "id": "def4042e",
    "outputId": "0171254c-ddeb-4ebb-cea4-af741ee8b03f"
   },
   "outputs": [],
   "source": [
    "# 10 most frequent bigrams \n",
    "bigram_table_10.plot('Bigrams','Frequency',kind='bar', width=0.6, color ='orange', title ='10 most frequent Bigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbfac53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "7dbfac53",
    "outputId": "958c92a4-3557-45aa-8dc8-e7e1a4c48989"
   },
   "outputs": [],
   "source": [
    "#trigrams = ngrams(corpus.split(), 3)\n",
    "#for item in trigrams:\n",
    "#    print(item)\n",
    "\n",
    "#creating dicitionary of trigrams with their frequency\n",
    "trigrams = ngrams(corpus.split(), 3)\n",
    "\n",
    "trigram_dict = {}\n",
    "for item in trigrams:\n",
    "    #print(item)\n",
    "    itemText = item[0]+' '+item[1]+' '+item[2]\n",
    "    if itemText not in trigram_dict.keys():\n",
    "        trigram_dict[itemText] = 1\n",
    "    else:    \n",
    "        trigram_dict[itemText] +=1 \n",
    "    \n",
    "trigram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eea32d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "hidden": true,
    "id": "4eea32d2",
    "outputId": "7cde7c8a-ece6-4b2e-b564-ad115ebd0f31"
   },
   "outputs": [],
   "source": [
    "#converting dictionary to dataframe for sorting and plotting\n",
    "list_trigram_key = list(trigram_dict.keys()) #converts dictionary keys to list of keys\n",
    "list_trigram_val = list(trigram_dict.values()) # converts dictionary values to list of values\n",
    "\n",
    "# creating table for noun words frequency\n",
    "trigram_table = pd.DataFrame()\n",
    "trigram_table['Trigrams']    = list_trigram_key\n",
    "trigram_table['Frequency']  = list_trigram_val\n",
    "trigram_table.sort_values(by='Frequency', ascending=False, inplace=True)\n",
    "trigram_table_10            = trigram_table.head(10)\n",
    "trigram_table_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7545c1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "hidden": true,
    "id": "f7545c1f",
    "outputId": "b39e93e1-b1ff-405c-80b2-5b03e53275bd"
   },
   "outputs": [],
   "source": [
    "# 10 most frequent trigrams \n",
    "trigram_table_10.plot('Trigrams','Frequency', kind='bar', width=0.6, title ='10 most frequent Trigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a4c97d",
   "metadata": {
    "heading_collapsed": true,
    "id": "72a4c97d"
   },
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b9f913",
   "metadata": {
    "hidden": true,
    "id": "00b9f913"
   },
   "source": [
    "#### Using Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7254d884",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "hidden": true,
    "id": "7254d884",
    "outputId": "71b139fa-15a0-4151-fc59-132cad7332f7"
   },
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(\n",
    "                        background_color='black',\n",
    "                        stopwords=stopwords,\n",
    "                        max_words=200, \n",
    "                        random_state=42).generate(str(noun_table))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Words frequented in text\", fontsize=15)\n",
    "plt.imshow(wordcloud.recolor(colormap= 'viridis' , random_state=42), alpha=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7976df8b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "hidden": true,
    "id": "7976df8b",
    "outputId": "c8c8e7f0-bfe8-4eb9-ac59-4a03b6001458"
   },
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(\n",
    "                        background_color='black',\n",
    "                        stopwords=stopwords,\n",
    "                        max_words=200, \n",
    "                        random_state=42).generate(str(verb_table))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Words frequented in text\", fontsize=15)\n",
    "plt.imshow(wordcloud.recolor(colormap= 'viridis' , random_state=42), alpha=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb9227f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "hidden": true,
    "id": "6cb9227f",
    "outputId": "e8769753-16b2-4605-b9ef-68b310651260"
   },
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(\n",
    "                        background_color='black',\n",
    "                        stopwords=stopwords,\n",
    "                        max_words=200, \n",
    "                        random_state=42).generate(str(word_count_t))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Words frequented in text\", fontsize=15)\n",
    "plt.imshow(wordcloud.recolor(colormap= 'viridis' , random_state=42), alpha=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598db9a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "hidden": true,
    "id": "598db9a5",
    "outputId": "55234f13-ec84-48f9-82a6-014356a2ed6b"
   },
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(\n",
    "                        background_color='black',\n",
    "                        stopwords=stopwords,\n",
    "                        max_words=200, \n",
    "                        random_state=42).generate(str(bigram_table))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Words frequented in text\", fontsize=15)\n",
    "plt.imshow(wordcloud.recolor(colormap= 'viridis' , random_state=42), alpha=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4d7c0d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "hidden": true,
    "id": "4d4d7c0d",
    "outputId": "aa8f9fd0-0b0d-4179-a99a-aadf20369d6c"
   },
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(\n",
    "                        background_color='black',\n",
    "                        stopwords=stopwords,\n",
    "                        max_words=200, \n",
    "                        random_state=42).generate(str(trigram_table))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Words frequented in text\", fontsize=15)\n",
    "plt.imshow(wordcloud.recolor(colormap= 'viridis' , random_state=42), alpha=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c9781",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Pretrained Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43de4759",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "print(len(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2eaeb2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Summarizer()\n",
    "summary = model(corpus) #sending entire text as string\n",
    "print(len(summary))\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb33165b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.run_embeddings(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4314796",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# specific summary length\n",
    "start_time = date.datetime.now()\n",
    "model = Summarizer()\n",
    "summary = model(corpus, num_sentences=6) #summary will have only 6 sentences\n",
    "end_time = date.datetime.now()\n",
    "time_taken = end_time - start_time\n",
    "print(\"Time taken for generating summary =\",time_taken)\n",
    "print(len(summary))\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f1d1dc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def summarysentiment(textsummary):\n",
    "    list_assert_words = []\n",
    "    afn = Afinn()\n",
    "    scores = afn.score(textsummary)\n",
    "    list_assert_words = afn.find_all(textsummary)\n",
    "    sentiment = ''\n",
    "    if scores > 0:\n",
    "        sentiment = 'Positive'\n",
    "    elif scores < 0:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    \n",
    "    return (sentiment,list_assert_words,scores)\n",
    "\n",
    "sentiment_summary, listAssertWords,scores = summarysentiment(summary)\n",
    "print(listAssertWords)\n",
    "print(sentiment_summary)\n",
    "print(\"summary review score =\",scores)\n",
    "print(\"Summary review is \",sentiment_summary)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bea23c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Word2Vec with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0f096",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff64a72a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#pip show gensim\n",
    "#!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcba2e7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df681702",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#creating list of sentences\n",
    "list_sentences = sent_tokenize(corpus)\n",
    "print(\"Total no of sentences = \",len(list_sentences),\"\\n\")\n",
    "#list_sentences\n",
    "list_10 = list_sentences[0:10]\n",
    "list_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec07fff5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# converting each sentence in list to word tokens\n",
    "filtered_sent =[]\n",
    "for sent in list_sentences:\n",
    "    new_sent = simple_preprocess(sent) \n",
    "    filtered_sent.append(new_sent)\n",
    "    \n",
    "print(len(filtered_sent))\n",
    "print(filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295a94e3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#passing the parameters for training the model\n",
    "#window 5 means 5 words before and after the target word\n",
    "#workers = no. of threads cpu will use for training\n",
    "#min_count=2 means sentence with minimum 2 or more words will be considered\n",
    "\n",
    "#model = Word2Vec(filtered_sent,min_count=2, window=5, iter=5)\n",
    "model = Word2Vec(min_count=2, window=5, workers=3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d1b43",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#unique list of words is build vocab\n",
    "# so it has initialized the model\n",
    "model.build_vocab(filtered_sent,progress_per=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741dd861",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"no of epochs =\",model.epochs) #how many times it iterate thorough the dataset\n",
    "print(\"no of sentences = \" ,model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eea050",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#training the model \n",
    "model.train(filtered_sent,total_examples=model.corpus_count,epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7e7dc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#saves model in jupyter notebook\n",
    "model.save('word2vectrain_book.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707df2f3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#getting vector representation of the word\n",
    "model.wv.get_vector('capabilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d23e3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words = list(model.wv.vocab)\n",
    "print(len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f5a25d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar('capabilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa8f451",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.wv.similarity(w1='capabilities',w2='customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b92230",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.wv.similarity(w1='capabilities',w2='capabilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9fb9fe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#gets top 5 similar words to create\n",
    "model.wv.most_similar('create',topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3010d48",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#loading the trained model from jupyter notebook\n",
    "new_pretrained_model = Word2Vec.load('word2vectrain_book.bin')\n",
    "print(new_pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec21aa55",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "new_pretrained_model.wv.most_similar('capabilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17cb50",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(new_pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b8ffce",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for each vocab (total 406) converting the 100 vectors to only 2 vector components\n",
    "X = new_pretrained_model[new_pretrained_model.wv.vocab]\n",
    "#print(X)\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "print(result.shape)\n",
    "print(result)\n",
    "#print(result[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a51b7df",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(result[:, 0], result[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9cbb93",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words = list(new_pretrained_model.wv.vocab)\n",
    "words_10 = words[0:10]\n",
    "print(words_10)\n",
    "\n",
    "for i, word in enumerate(words_10):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc9a41d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Extractive Summarization - Spacy - word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e08d307",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c77d509",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c57070",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"length of corpus =\",len(corpus))\n",
    "#passing entire corpus(string)\n",
    "doc = nlp(corpus)\n",
    "print(\"length of doc =\",len(doc))\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb06f2f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#print tokens from the doc\n",
    "word_tokens = [tokens.text for tokens in doc]\n",
    "print(\"length of tokens\", len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630ba9ac",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spacy_stopwords = list(STOP_WORDS)\n",
    "print(spacy_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6bfab9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#removing punctuation and stopwords\n",
    "#creates word frequency dictionary with words(keys) thier frequency(values)\n",
    "word_frequencies = {} \n",
    "for word in word_tokens:\n",
    "    if word not in punctuation:\n",
    "        if word not in spacy_stopwords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] +=1\n",
    "\n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0cbb57",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_frequency = max(word_frequencies.values())\n",
    "max_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a4191a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#we will divide each value in word_frequency dict by maximum frequency(40)\n",
    "#to get a normalized value for all words\n",
    "#so, 40/40 = 1, is the maximum normalized frequency\n",
    "\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = word_frequencies[word]/max_frequency\n",
    "    \n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285eeebe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentence_tokens = [sent for sent in doc.sents]\n",
    "print(\"length of sentence tokens =\",len(sentence_tokens),\"\\n\")\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46fc30d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#to calculate sentence scores\n",
    "#print(word_frequencies)\n",
    "sentence_scores = {}\n",
    "\n",
    "for sent in sentence_tokens:\n",
    "        for word in sent: # for each word in sentence\n",
    "            if word.text.lower() in word_frequencies.keys(): # if word exists in word frequency dict\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word.text.lower()]#freq. of word from word_freq dict assig to sent_score dict\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word.text.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd6078e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf2507",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#30% of the sentences from the entire corpus will be selected\n",
    "# ie 30% of 173 sentences = 51 sentence\n",
    "select_length = int(len(sentence_tokens)*0.03)\n",
    "select_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42810abd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#will select sentences(select_length=5) with highest sentence score\n",
    "summary = nlargest(select_length,sentence_scores,key=sentence_scores.get)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55117d3a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sys_sum_spacy = ' '.join([str(elem) for elem in summary])\n",
    "print(\"Summary :\\n\",sys_sum_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47de1e3f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Extractive Summarization - Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89685bfd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize, keywords\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505b997c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "summary = summarize(corpus, word_count=100)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc75e873",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Abstractive Summarization - T5 Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27033e5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json \n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b61172",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_t5 = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "device = torch.device('cpu')  # model was trained on GPU, need to do when running on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f69bf6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preprocess_text = corpus.strip().replace(\"\\n\",\"\")\n",
    "t5_prepared_Text = \"summarize: \"+preprocess_text # need to add 'summarize:' to input text data\n",
    "print (\"original text preprocessed: \\n\", preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fefdaf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# summmarize \n",
    "summary_ids = model_t5.generate(tokenized_text,\n",
    "                                    num_beams=4,\n",
    "                                    no_repeat_ngram_size=2,\n",
    "                                    min_length=100,\n",
    "                                    max_length=300,\n",
    "                                    early_stopping=True)\n",
    "\n",
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print (\"\\n\\nSummarized text: \\n\",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635cbe05",
   "metadata": {},
   "source": [
    "###  Extractive Summarization - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbcdf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence tokenization from text \n",
    "sentences  = sent_tokenize(corpus)\n",
    "print(sentences)\n",
    "total_documents = len(sentences)\n",
    "total_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b057d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate frequency of words in each sentence\n",
    "def create_frequency_matrix(sentences):\n",
    "    frequency_matrix = {}\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    for sent in sentences:\n",
    "        #print(sent,\"\\n\")\n",
    "        freq_table = {}\n",
    "        words = word_tokenize(sent)\n",
    "        #print(words,\"\\n\")\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = ps.stem(word)\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "            \n",
    "            #print(word)\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "                \n",
    "        frequency_matrix[sent[:15]] = freq_table\n",
    "        \n",
    "    return frequency_matrix\n",
    "\n",
    "#Create the Frequency matrix of the words in each sentence.\n",
    "freq_matrix = create_frequency_matrix(sentences)\n",
    "print(freq_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566d4a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating term frequency\n",
    "#Here, the document is a sentence, the term is a word in sentence\n",
    "def create_tf_matrix(freq_matrix):\n",
    "    tf_matrix = {}\n",
    "    \n",
    "    for sent,f_table in freq_matrix.items():\n",
    "        #print(f_table)\n",
    "        tf_table = {}\n",
    "        count_words_sentence = len(f_table)\n",
    "        #print(count_words_sentence)\n",
    "        for word, count in f_table.items():\n",
    "            tf_table[word] = count/count_words_sentence\n",
    "\n",
    "        tf_matrix[sent] = tf_table\n",
    "            \n",
    "    return tf_matrix \n",
    "\n",
    "#calculate the term frequency and generate a matrix\n",
    "tf_matrix = create_tf_matrix(freq_matrix)\n",
    "print(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6944375",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating how many sentences contains a word\n",
    "def create_documents_per_words(freq_matrix):\n",
    "    word_per_doc_table = {}\n",
    "    \n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        #print(sent)\n",
    "        #print(f_table)\n",
    "        for word, count in f_table.items():\n",
    "            if word in word_per_doc_table:\n",
    "                #print(word)\n",
    "                word_per_doc_table[word] +=1\n",
    "            else:\n",
    "                word_per_doc_table[word] = 1\n",
    "                \n",
    "    return word_per_doc_table\n",
    "\n",
    "#creating table for documents per words\n",
    "count_doc_per_words  = create_documents_per_words(freq_matrix)\n",
    "print(count_doc_per_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating IDF for each word in a sentence\n",
    "def create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
    "    idf_matrix = {}\n",
    "    \n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        idf_table = {}\n",
    "        #print(sent)\n",
    "\n",
    "        for word in f_table.keys():\n",
    "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
    "\n",
    "        idf_matrix[sent] = idf_table\n",
    "\n",
    "    return idf_matrix\n",
    "\n",
    "# calculate IDF and generate a matrix\n",
    "idf_matrix = create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
    "print(idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46104bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating TF-IDF (i.e TF*IDF)\n",
    "def create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "\n",
    "        tf_idf_table = {}\n",
    "\n",
    "        for (word1, value1), (word2, value2) in zip(f_table1.items(),f_table2.items()):  # here, keys are the same in both the table\n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "\n",
    "        tf_idf_matrix[sent1] = tf_idf_table\n",
    "\n",
    "    return tf_idf_matrix\n",
    "\n",
    "# Calculate TF-IDF and generate a matrix\n",
    "tf_idf_matrix = create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
    "print(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b9d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scoring sentences\n",
    "def score_sentences(tf_idf_matrix) -> dict:\n",
    "    sentenceValue = {}\n",
    "\n",
    "    for sent, f_table in tf_idf_matrix.items():\n",
    "        total_score_per_sentence = 0\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, score in f_table.items():\n",
    "            total_score_per_sentence += score\n",
    "\n",
    "        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence\n",
    "\n",
    "    return sentenceValue\n",
    "\n",
    "#score the sentences\n",
    "sentence_scores = score_sentences(tf_idf_matrix)\n",
    "print(sentence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating threshold = average sentece score\n",
    "def find_average_score(sentenceValue) -> int:\n",
    "    \n",
    "    sumValues = 0\n",
    "    for entry in sentenceValue:\n",
    "        sumValues += sentenceValue[entry]\n",
    "\n",
    "    # Average value of a sentence from original summary_text\n",
    "    average = (sumValues / len(sentenceValue))\n",
    "\n",
    "    return average\n",
    "\n",
    "#Find the threshold\n",
    "threshold = find_average_score(sentence_scores)\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a3dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select a sentence if the sentence score is greater than threshold\n",
    "def generate_summary(sentences, sentenceValue, threshold):\n",
    "    sentence_count = 0\n",
    "    summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        #print(sentence)\n",
    "       # print(sentenceValue)\n",
    "        #print(threshold)\n",
    "        if sentence[:15] in sentenceValue and sentenceValue[sentence[:15]] >= (threshold):\n",
    "        #if sentenceValue[sentence[:15]] >= (threshold):\n",
    "            #print(\"inside if\")\n",
    "            #print(sentenceValue[sentence[:15]])\n",
    "            #print(sentenceValue[sentence],\"\\n\")\n",
    "            summary += \" \" + sentence\n",
    "            sentence_count += 1\n",
    "            exit()\n",
    "\n",
    "    #print(\"==============\",summary)\n",
    "    return summary\n",
    "\n",
    "#Generate the summary\n",
    "summary = generate_summary(sentences, sentence_scores, 1.5 * threshold)\n",
    "print(\"summary:\\n\",summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff14c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Abstract_summary_generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
